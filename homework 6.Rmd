---
title: "Homework 6"
author: "Olivia Wagner"
date: "11/21/2019"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(knitr)
library(modelr)
```

```{r problem 1 models}
birthweight_data = janitor::clean_names(read_csv('./birthweight.csv'))

print(birthweight_data)

# Data Clean #

missing  = apply(birthweight_data, 2, function(x) any(is.infinite(x) | is.na(x)))
print(missing)

birthweight_data = birthweight_data %>%
  mutate(mrace = as.factor(mrace), malform = as.factor(malform), frace = as.factor(frace), babysex = as.factor(babysex)) 

# Hypothesized Model #

initial_model = lm(bwt ~ ., data = birthweight_data)
summary(initial_model)

hyp_model = lm(bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + mrace + parity + ppwt + blength*bhead + delwt*ppwt, data = birthweight_data)
summary(hyp_model)

# residual plot #

hyp_plot = birthweight_data %>% 
  add_residuals(hyp_model) %>%
  add_predictions(hyp_model) %>%
  ggplot(aes(x = pred, y = resid)) +
  geom_point(aes(alpha = 0.30))

print(hyp_plot)

# model 2 #

birth_age_model = lm(bwt ~ blength + gaweeks, data = birthweight_data)
summary(birth_age_model)

# model 3 #

length_gender_model = lm(bwt ~ bhead + babysex + blength + bhead*blength + bhead*babysex + babysex*blength + babysex*blength*bhead, data = birthweight_data)
summary(length_gender_model)

```

 To create my hypothesized model, I initially fit a saturated model then pared down the number of variables to those that had a p-value of 0.20 or less, and created interactions for varaibles whose measurements seemed to be correlated. In plotting the residuals vs. the predicted values above we see most of the variation seems to be scattered fairly evenly across resid = 0, with the exception of a few values in the left tail of the data. 
 
 

```{r problem 1 CV}
# cross validation #
cv_df = crossv_mc(birthweight_data, 100)
cv_df = cv_df %>% mutate(train = map(train, as_tibble, test = map(as_tibble)))

cv_df = cv_df %>% 
  mutate(hyp_mod = map(train, ~lm(bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + mrace + parity + ppwt + blength*bhead + delwt*ppwt, data = .x)), 
         age_mod = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)), 
         length_mod = map(train, ~lm(bwt ~ bhead + babysex + blength + bhead*blength + bhead*babysex + babysex*blength + babysex*blength*bhead, data = .x)))%>%
  mutate(rmse_hyp = map2_dbl(hyp_mod, test, ~rmse(model = .x, data = .y)), 
         rmse_age = map2_dbl(age_mod, test, ~rmse(model = .x, data = .y)), 
         rmse_length = map2_dbl(length_mod, test, ~rmse(model = .x, data = .y)))


# prediction error plot #
prediction_error = cv_df%>%
  select(starts_with('rmse')) %>%
  pivot_longer(everything(), names_to = 'model', values_to = 'rmse', names_prefix = 'rmse_') %>%
  mutate(model = fct_inorder(model)) %>%
  mutate(model = fct_inorder(model)) %>%
  ggplot(aes(x = model, y = rmse)) +
  geom_violin() %>% 
  print()
  

```

The best performing model was my hypothesized model. The amount of cross validation prediction error is far lower than the other two models (birth/gestational age and head/length measurement models). The adjusted p-value is also greater then the other two models.



```{r problem 2}

weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())


# bootstrap #

boot_straps = weather_df %>%
  modelr::bootstrap(n = 5000)

# work on log(beta0 * beta1) #
boot_strap_df = boot_straps %>% mutate(models = map(strap, ~lm(tmax ~ tmin, data = .x)), results = map(models, broom::tidy)) %>%
  select(-strap, -models) %>%
  unnest(results) %>%
  group_by(.id) %>%
  mutate(min_est = ifelse(term == 'tmin', estimate, 0), int_est = ifelse(term == '(Intercept)', estimate, 0)) 
  
log_param = boot_strap_df %>%
  group_by(.id) %>%
  summarize(min_est = sum(min_est), int_est = sum(int_est)) %>%
  mutate(log_min_max = log(min_est*int_est)) %>%
  print()

# work on r^2 #

boot_strap_df_corr = boot_straps %>% mutate(models = map(strap, ~lm(tmax ~ tmin, data = .x)), results = map(models, broom::glance)) %>%
  select(-strap, -models) %>%
  unnest(results) %>% 
  select(.id, r.squared)

# plot distributions #

ggplot(data = boot_strap_df_corr, aes(x = r.squared)) +
  geom_histogram(aes(y = ..density..), color = 'black', fill = 'white')+
  geom_density(alpha = 0.25, fill = '#FF6666')+
  ggtitle('Distribution of R squared')

ggplot(data = log_param, aes(x = log_min_max)) +
  geom_histogram(aes(y = ..density..), color = 'black', fill = 'white')+
  geom_density(alpha = 0.25, fill = '#FF6666')+
  ggtitle('Distribution of log(beta_0 * beta_1)')

# Confidence Intervals #
    # R squared upper bound #
    quantile(pull(boot_strap_df_corr, r.squared), 0.975)
    # R squared lower bound #
    quantile(pull(boot_strap_df_corr, r.squared), 0.025)
    
    
    # log estimate upper bound #
    quantile(pull(log_param, log_min_max), 0.975)
    #log estimate lower bound #
    quantile(pull(log_param, log_min_max), 0.025)

```
The distribution of both of these estimates appear to be approximately normal, with the distribution of r squared values slightly skewed to the left. 

R squared 95% CI : (0.8946, 0.9272)

log($\hat{\beta_{0}}*\hat{\beta_{1}}$) 95% CI: (1.964, 2.058)